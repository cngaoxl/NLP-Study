{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(2.9019, grad_fn=<NegBackward>)\n",
      "loss:  tensor(2.8068, grad_fn=<NegBackward>)\n",
      "loss:  tensor(2.8553, grad_fn=<NegBackward>)\n",
      "loss:  tensor(2.9406, grad_fn=<NegBackward>)\n",
      "loss:  tensor(2.7972, grad_fn=<NegBackward>)\n",
      "loss:  tensor(2.7880, grad_fn=<NegBackward>)\n",
      "loss:  tensor(2.8808, grad_fn=<NegBackward>)\n",
      "loss:  tensor(2.8411, grad_fn=<NegBackward>)\n",
      "loss:  tensor(2.7883, grad_fn=<NegBackward>)\n",
      "tensor([[ 0.2576, -0.4979],\n",
      "        [ 0.2576, -0.4979],\n",
      "        [ 0.2576, -0.4979],\n",
      "        [ 0.2576, -0.4979],\n",
      "        [ 0.2576, -0.4979],\n",
      "        [-0.6731,  0.6583],\n",
      "        [-0.6731,  0.6583],\n",
      "        [-0.2405,  0.0640],\n",
      "        [-0.2405,  0.0640],\n",
      "        [-0.2405,  0.0640],\n",
      "        [-0.2405,  0.0640],\n",
      "        [-0.2405,  0.0640],\n",
      "        [-0.2466, -0.8853],\n",
      "        [-0.2466, -0.8853],\n",
      "        [ 0.4153, -0.1400],\n",
      "        [ 0.4153, -0.1400],\n",
      "        [ 0.4153, -0.1400],\n",
      "        [ 0.4153, -0.1400],\n",
      "        [ 0.1676, -0.0508],\n",
      "        [ 0.1676, -0.0508],\n",
      "        [ 0.1676, -0.0508],\n",
      "        [ 0.1676, -0.0508],\n",
      "        [ 0.1676, -0.0508]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "#简单的skip-gram在训练300维度*10000个词时会很慢，因为会有3亿个weigth要去训练\n",
    "#本文采用二次采样的方法\n",
    "#1. Subsampling frequent words to decrease the number of training examples.\n",
    "#2. Modifying the optimization objective with a technique they called “Negative Sampling”, \n",
    "#which causes each training sample to update only a small percentage of the model’s weights.\n",
    "\n",
    "#ref https://programmer.group/pytorch-implements-word2vec.html\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from collections import Counter\n",
    "#import matplotlib.pyplot as plt\n",
    "embedding_dim = 2 #词嵌入的维度\n",
    "print_every = 100\n",
    "epochs = 100      #训练多少epochs\n",
    "batch_size = 6    #batch_size\n",
    "N_samples = 3\n",
    "window_size = 5   #窗口大小\n",
    "threshhold = 0\n",
    "keep_threshold = 0.001 #分数值，通过公式计算出来的概率值大于该值时单词保留\n",
    "a = 0.001         #计算单词保留分数时使用，越小保留的单词越少\n",
    "\n",
    "def basic_preprocess(text,freq):\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    \n",
    "    #单词为key，单词出现次数为value的词典\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    #仅保留单词个数大于freq的词\n",
    "    trimmed_words = [word for word in words if word_counts[word] > freq]\n",
    "    return  trimmed_words\n",
    "    \n",
    "text = 'Running gradient descent on a neural network that large is going to be slow. And to make matters worse, you need a huge amount of training data in order to tune that many weights and avoid over-fitting. millions of weights times billions of training samples means that training this model is going to be a beast'\n",
    "#语料比较小，threshhold为0，所有词都保留\n",
    "words = basic_preprocess(text,threshhold)\n",
    "\n",
    "#去掉重复的词\n",
    "vocabulary = set(words)\n",
    "\n",
    "#单词为key，index为value的词典\n",
    "#enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列,同时列出数据和数据下标\n",
    "vocabulary2index = {w:c for c,w in enumerate(vocabulary)}\n",
    "\n",
    "#index为key，单词为value的词典\n",
    "index2vocabulary = {c:w for c,w in enumerate(vocabulary)}\n",
    "\n",
    "\n",
    "#将所有单词转换为单词对应的index，用index来表示语料\n",
    "index_words = [vocabulary2index[w] for w in words]\n",
    " \n",
    "#单词index为key，单词出现次数为value的词典\n",
    "word_count = Counter(index_words)\n",
    "\n",
    "#词典长度，有多少个单词\n",
    "total_count = len(word_count)\n",
    "\n",
    "#单词index为key，词频为value的词典\n",
    "word_freqs = {w:c/total_count for w,c in word_count.items()} \n",
    "\n",
    "#----------------------------------------------sampling rate------------------------------------------\n",
    "#计算词被保留的分数，该公式课参考论文。\n",
    "#公式 (sqrt(x/0.001) + 1) / 0.001 / x, 其中x为单词的频率(单词出现的次数/ 语料中总的单词数)\n",
    "#0.001就是下式中的a，该值越小表示保留该词的可能性越小\n",
    "prob_keep = {w:np.sqrt(word_freqs[w]/a + 1)* a /word_freqs[w]   for w in index_words}\n",
    "\n",
    "#分数大于keep_threshold的单词保留\n",
    "train_words = [w for w in index_words if prob_keep[w] >keep_threshold ]\n",
    "#----------------------------------------------sampling rate End---------------------------------------\n",
    "\n",
    "#----------------------------------------------Negative Sampling------------------------------------------\n",
    "#词频转换为ndarray\n",
    "word_freqs = np.array(list(word_freqs.values()))\n",
    "\n",
    "#每一个单词出现的频率\n",
    "unigram_dist = word_freqs / word_freqs.sum()\n",
    "\n",
    "#选择3/4是根据实验结果，3/4的效果好; 词频越高的词越容易被选择为负样本\n",
    "#每个词被采样的概率，论文中叫噪声点分布，噪声分布是指与输入单词无关得单词，一般从词汇表中随机提取以获得这些噪声\n",
    "#因此我们可以决定如何设置提取单词的权重。它可以是均匀分布，也就是说，提取所有单词的概率是相同的。也可以根据每个单词出现在文本语料库中的频率进行采样。根据作者的实践，最佳分布是3/4。\n",
    "noise_dist = torch.from_numpy(unigram_dist ** (0.75) / np.sum(unigram_dist ** (0.75)))\n",
    "\n",
    "#----------------------------------------------Negative Sampling End------------------------------------------\n",
    "\n",
    "\n",
    "#取得context_word\n",
    "#words：单词列表(1个batch的单词列表)\n",
    "#index:中心词的位置\n",
    "#widow_size: 中心词的前后window_size的词可作为context word\n",
    "def get_positive_words(words,index,window_size):\n",
    "    #返回一个随机整数\n",
    "    target_window = np.random.randint(1,window_size+1)\n",
    "    \n",
    "    #起始单词的位置\n",
    "    start_point = index-target_window if (index-target_window) > 0 else 0\n",
    "    #结束单词的位置\n",
    "    end_point = index + target_window\n",
    "    \n",
    "    #中心词的前n个单词和后n个单词，n<=window_size\n",
    "    targets = set(words[start_point:index] + words[index +1:end_point + 1])\n",
    "    return list(targets)\n",
    "\n",
    "#load一个batch的训练数据， x为中心词， y为上下文单词\n",
    "#words:单词的index\n",
    "#batch_size:一次返回batch_size大小的数据训练\n",
    "def batch_loader(words,batch_size,window_size):\n",
    "    \n",
    "    #需要多少个batch\n",
    "    n_batches = len(words)//batch_size\n",
    "    words = words[:n_batches*batch_size]\n",
    "    for idx in range(0,len(words),batch_size):\n",
    "        batch_x, batch_y = [],[]\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for i in range(len(batch)):\n",
    "            x = batch[i]\n",
    "            y = get_positive_words(batch,i,window_size)\n",
    "            batch_x.extend([x]*len(y))\n",
    "            batch_y.extend(y)\n",
    "        yield batch_x,batch_y\n",
    "\n",
    "#该类继承nn.Module, nn.Module是所有神经网络类的基类\n",
    "#ref https://pytorch.org/docs/stable/nn.html\n",
    "class SkipGramNeg(nn.Module):\n",
    "    '''\n",
    "    n_vocab:词典大小，单词个数\n",
    "    n_embed:词向量的维度\n",
    "    noise_dist：单词的采样概率(根据论文中的公式事前计算)\n",
    "    '''\n",
    "    #init函数构建神经网络\n",
    "    def __init__(self,n_vocab,n_embed,noise_dist):\n",
    "        super().__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.noise_dist = noise_dist\n",
    "        \n",
    "        #nn.Embedding: 存储一个固定词典大小和词向量维度的表，输入是一个词的index，输出是该词的词向量\n",
    "        #词向量，n_vocab:词表大小，n_embed:每个单词的维度\n",
    "        self.in_embed = nn.Embedding(n_vocab,n_embed)\n",
    "        \n",
    "         #词向量，n_vocab:词表大小，n_embed:每个单词的维度\n",
    "        self.out_embed = nn.Embedding(n_vocab,n_embed)\n",
    "        \n",
    "        #用-1到1之间的随机均匀分布来初始化权重\n",
    "        self.in_embed.weight.data.uniform_(-1,1)\n",
    "        self.out_embed.weight.data.uniform_(-1,1)\n",
    "\n",
    "    # 输入词，返回输入词的词向量\n",
    "    def forward_input(self,input_words):\n",
    "        input_vectors = self.in_embed(input_words)\n",
    "        return input_vectors\n",
    "    \n",
    "    #输入词，返回输入词对应的词向量\n",
    "    def forward_output(self,output_words):\n",
    "        output_vectors = self.out_embed(output_words)\n",
    "        return output_vectors\n",
    "    \n",
    "    #对上下文词的词向量进行负采样\n",
    "    #size：单词表的大小， N_sample\n",
    "    def forward_noise(self,size,N_sample):\n",
    "        noise_dist = self.noise_dist\n",
    "        \n",
    "        #从采样概率分布表中抽取size*N_sample   size是单词表大小\n",
    "        noise_words = torch.multinomial(noise_dist,\n",
    "                                        size * N_sample,\n",
    "                                        replacement=True)\n",
    "        noise_vectors = self.out_embed(noise_words).view(size, N_sample, self.n_embed)\n",
    "        return noise_vectors\n",
    "\n",
    "#负采样损失  \n",
    "class NegativeSampleLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    #前向传播函数\n",
    "    #损失函数参考https://programmer.group/pytorch-implements-word2vec.html，里面有详细说明\n",
    "    def forward(self,input_vectors,output_vectors,noise_vectors):\n",
    "    \n",
    "        batch_size,emded_size = input_vectors.shape\n",
    "        \n",
    "        # Input vectors should be a batch of column vectors\n",
    "        input_vectors = input_vectors.view(batch_size,emded_size,1)\n",
    "        \n",
    "        # Output vectors should be a batch of row vectors\n",
    "        output_vectors = output_vectors.view(batch_size,1, emded_size)# can be multiply with each other\n",
    "\n",
    "        # bmm = batch matrix multiplication\n",
    "        # correct log-sigmoid loss\n",
    "        out_loss = torch.bmm(output_vectors,input_vectors).sigmoid().log()\n",
    "        \n",
    "        #the size is [batchsize,1,1], so we need to reduce the dim\n",
    "        out_loss = out_loss.squeeze()\n",
    "\n",
    "        #loss value of negative word\n",
    "        noise_loss = torch.bmm(noise_vectors.neg(),input_vectors).sigmoid().log()\n",
    "        noise_loss = noise_loss.squeeze().sum(1) # sum the losses over the sample of noise vectors\n",
    "\n",
    "        return -(out_loss + noise_loss).mean()\n",
    "\n",
    "#创建model，参数分别为词典大小（单词个数），词向量的维度， 每个词的采样概率\n",
    "model = SkipGramNeg(len(vocabulary2index),embedding_dim,noise_dist)\n",
    "criterion = NegativeSampleLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.001)\n",
    "\n",
    "steps = 0\n",
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    #取中心词和上下文词\n",
    "    for input_words,target_word in batch_loader(train_words,batch_size,window_size):\n",
    "        steps +=1\n",
    "        inputs,targets = torch.LongTensor(input_words),torch.LongTensor(target_word)\n",
    "        \n",
    "        #中心词喂给model作为x，输出中心词的词向量，\n",
    "        #weight是在torch embdding（self.in_embed）内部作为属性保存着，刚开始时候用初期化的weight* inputs得到input_vectors\n",
    "        #后期随着训练，会更新self.in_embed中的weight，返回新的input_vectors\n",
    "        input_vectors = model.forward_input(inputs)\n",
    "        \n",
    "        #上下文词喂给model作为y，输出上下文词的词向量\n",
    "        output_vectors = model.forward_output(targets)\n",
    "        size,_ = input_vectors.shape\n",
    "        \n",
    "        noise_vectors = model.forward_noise(size,N_samples)\n",
    "        \n",
    "        #损失计算\n",
    "        loss = criterion(input_vectors,output_vectors,noise_vectors)\n",
    "        if steps%print_every ==0:\n",
    "            print('loss: ',loss)\n",
    "        \n",
    "        #保存的Variable grad清零，梯度清0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #会调用torch.autograd.backward(loss)，进行反向传播的梯度计算\n",
    "        loss.backward()\n",
    "        \n",
    "        #用上一步计算出的梯度去更新各层的权重\n",
    "        optimizer.step()\n",
    "\n",
    "print(input_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
