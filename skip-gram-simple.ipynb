{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epo 0: 4.5037336349487305\n",
      "Loss at epo 10: 3.858903169631958\n",
      "Loss at epo 20: 3.518991470336914\n",
      "Loss at epo 30: 3.3026914596557617\n",
      "Loss at epo 40: 3.145211935043335\n",
      "Loss at epo 50: 3.0224287509918213\n",
      "Loss at epo 60: 2.9229726791381836\n",
      "Loss at epo 70: 2.8402323722839355\n",
      "Loss at epo 80: 2.7698750495910645\n",
      "Loss at epo 90: 2.708897113800049\n"
     ]
    }
   ],
   "source": [
    "#refrence：https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb?gi=f106913ccce5\n",
    "#https://zhangweifeng.top/2019/08/03/%E4%BD%BF%E7%94%A8Pytorch%E5%AE%9E%E7%8E%B0word2vec-skip-gram/ 中文说明\n",
    "#Skip-gram的简单实现，不涉及负采样\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "corpus = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'warsaw is poland capital',\n",
    "    'berlin is germany capital',\n",
    "    'paris is france capital',\n",
    "]\n",
    "\n",
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "\n",
    "#print(tokenized_corpus)\n",
    "\n",
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "#print(word2idx)\n",
    "#print(idx2word)\n",
    "\n",
    "window_size = 2\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    #print(indices)\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "#print(idx_pairs)\n",
    "\n",
    "#Input layer is just the center word encoded in one-hot manner. It dimensions are [1, vocabulary_size]\n",
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "    \n",
    "#Hidden layer makes our v vectors. Therefore it has to have embedding_dims neurons. To compute it value we have to define W1 weight matrix. Of course its has to be #[embedding_dims, vocabulary_size]. There is no activation function — just plain matrix multiplication.\n",
    "#embedding_dims = 5\n",
    "#W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "#z1 = torch.matmul(W1, x)\n",
    "\n",
    "#Last layer must have vocabulary_size neurons — because it generates probabilities for each word. Therefore, W2 is [vocabulary_size, embedding_dims] in terms of shape.\n",
    "#W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "#z2 = torch.matmul(W2, z1)\n",
    "\n",
    "#On top on that we have to use softmax layer. PyTorch provides optimized version of this, combined with log — because regular softmax is not really numerically stable:\n",
    "#log_softmax = F.log_softmax(a2, dim=0)\n",
    "\n",
    "#Now we can compute loss. As usual PyTorch provides everything we need:\n",
    "#loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "#The nll_loss computes negative-log-likelihood on logsoftmax. y_true is context word — we want to make this as high as possible\n",
    "# — because pair x, y_true is from training data — so the are indeed center, context.\n",
    "\n",
    "#As we fished forward pass, now it’s time to perform backward pass. Simply:\n",
    "#loss.backward()\n",
    "\n",
    "#For optimization SDG is used. It is so simple, that it was faster to write it by hand instead of creating optimizer object:\n",
    "#W1.data -= 0.01 * W1.grad.data\n",
    "#W2.data -= 0.01 * W2.grad.data\n",
    "\n",
    "#Last step is to zero gradients to make next pass clear:\n",
    "#W1.grad.data.zero_()\n",
    "#W2.grad.data.zero_()\n",
    "\n",
    "#Training loop\n",
    "#Time to compile it into training loop. It can look like:\n",
    "\n",
    "embedding_dims = 5\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    \n",
    "    #data:the index of centerword\n",
    "    #target:the index of context word\n",
    "    for data, target in idx_pairs:\n",
    "        #初始化x为字典大小的维度，且该词所在的位置设为1， 相当于对x进行one hot编码\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        \n",
    "        #context word的index作为y值\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        #(embedding_dims, vocabulary_size) * (vocabulary_size,1)\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        #z1:(embedding_dims,1)\n",
    "        \n",
    "        #(vocabulary_size, embedding_dims) * (embedding_dims,1)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "        #z2:(vocabulary_size,1)\n",
    "\n",
    "        #对z2进行sfotmax后计算log\n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "        \n",
    "        #负对数似然\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.data\n",
    "        \n",
    "        #当我们调用loss.backward()函数的时候，整张图都被依次计算误差，所有Variable的.grad属性会被累加．\n",
    "        loss.backward()\n",
    "        \n",
    "        #根据梯度改变权重\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "        \n",
    "        #对现有梯度清零\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "    if epo % 10 == 0:\n",
    "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取向量\n",
    "现在我们训练了一个网络，最后一件事就是提取每个单词的向量，这里有三个可能的方式\n",
    "- 使用W1的v向量\n",
    "- 使用W2的u向量\n",
    "- 使用u和v的平均\n",
    "你可以自己思考什么时候用哪个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2192, -0.3107,  0.1193,  0.5108, -0.0473, -1.0777,  0.2223, -1.2885,\n",
      "          1.1545, -0.0242,  0.5189,  0.4307, -0.8447, -0.4843,  0.4388],\n",
      "        [-0.5908, -0.4645,  1.0955, -0.3151,  0.3382,  0.7444,  0.3455, -0.5546,\n",
      "          0.3973, -0.6931,  1.5637, -1.0222,  0.4685,  1.4698,  0.9312],\n",
      "        [-0.6092, -0.3868, -0.7027, -1.6485, -0.1471,  0.8466,  0.6977, -0.4477,\n",
      "         -0.0858, -1.2249,  0.4202,  0.6882,  1.5914,  1.0136,  0.8029],\n",
      "        [ 0.8918, -0.4556,  0.5177,  0.1766,  0.4958,  0.1987,  1.3939, -0.4566,\n",
      "          1.7754, -0.8591,  0.7134, -1.4443,  0.5845, -0.4377, -0.7417],\n",
      "        [-0.8782,  0.2848, -1.1601,  0.6272,  0.3698,  0.3644, -1.4403,  0.1981,\n",
      "         -0.5880,  1.3435,  0.0704,  0.6243, -0.6992,  0.6870,  0.9619]],\n",
      "       requires_grad=True)\n",
      "{0: 'he', 1: 'is', 2: 'a', 3: 'king', 4: 'she', 5: 'queen', 6: 'man', 7: 'woman', 8: 'warsaw', 9: 'poland', 10: 'capital', 11: 'berlin', 12: 'germany', 13: 'paris', 14: 'france'}\n",
      "tensor([-0.2192, -0.5908, -0.6092,  0.8918, -0.8782], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "#使用W1的v向量\n",
    "print(W1)\n",
    "print(idx2word)\n",
    "#w1的列就是每个词对应的vect\n",
    "def get_vect_by_word(word):\n",
    "    index = word2idx[word]\n",
    "    return W1[:,index]\n",
    "print(get_vect_by_word('he'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0457e+00, -1.0377e+00,  4.8538e-01, -5.0248e-01, -1.9111e+00],\n",
      "        [ 1.0480e-01,  1.4736e+00,  7.1006e-01,  2.0185e+00,  4.1934e-01],\n",
      "        [-1.7788e-01, -1.1168e+00, -1.4770e-01,  8.1876e-01,  9.5763e-01],\n",
      "        [-1.4697e+00,  4.1635e-01, -2.7296e-01,  1.4065e+00,  4.5451e-01],\n",
      "        [ 2.5076e-01,  1.4806e+00,  1.0296e+00, -5.4036e-01, -1.0844e+00],\n",
      "        [ 1.3061e+00,  1.8377e-01,  5.5527e-01,  6.5548e-01,  7.4693e-02],\n",
      "        [-4.8760e-01, -2.9554e-01, -4.3771e-01, -3.1433e-01, -1.4220e+00],\n",
      "        [-1.0272e+00,  4.0388e-01,  8.9364e-01, -1.2516e+00, -8.8262e-01],\n",
      "        [ 1.5421e+00, -2.0172e-01, -4.2868e-01, -5.6718e-01, -3.4759e-01],\n",
      "        [ 7.5248e-01,  7.6497e-01,  3.4004e-02,  3.7912e-01, -3.0912e-01],\n",
      "        [-9.6537e-01, -7.3450e-02, -4.9049e-02,  1.9782e+00,  6.6227e-01],\n",
      "        [-8.2628e-01, -1.3237e-01, -5.8901e-01,  1.3122e+00,  5.0472e-01],\n",
      "        [ 3.1152e+00, -1.3643e+00,  3.7410e-01,  2.6550e-03, -1.2258e+00],\n",
      "        [-2.2485e-01, -2.1324e-01,  6.8668e-01,  6.9844e-01,  5.6493e-01],\n",
      "        [ 5.7617e-01,  8.3405e-01,  7.2067e-01, -7.2876e-01,  3.8256e-01]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.1048, 1.4736, 0.7101, 2.0185, 0.4193], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用W2的u向量\n",
    "print(W2)\n",
    "def get_vect_by_word_from_w2(word):\n",
    "    index = word2idx[word]\n",
    "    return W2[index]\n",
    "get_vect_by_word_from_w2('is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1029,  0.5046,  0.1616,  0.7815,  0.3521], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用u和v的平均\n",
    "(get_vect_by_word('is')+get_vect_by_word_from_w2('is'))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
